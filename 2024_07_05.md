신경망: 머신러닝 중 하나, 최근에는 신경망(딥러닝)이 다른 머신러닝보디 뛰어나다는 평가를 받고 있음.

초기 신경망

    퍼셉트론: 다수의 입력 -> 하나의 결과를 나타내는 구조, 가중치를 조절해서 답을 찾음(가중치가 높으면 그 입력값이 중요하다는 의미)
              
              위의 결과 출력값이 특정 임계값을 넘을 경우 1 아닐 경우 0으로 나옴.
              
              그 외에 편향과 활성화 함수가 존재함.

    단층 퍼셉트론(Single-Layer Perceptron): 입력층과 출력층으로 구성된 단순한 퍼셉트론

                  단층 퍼셉트론은 xor게이트를 표현할 수 없어서 딥러닝에 적합하지 않음.

![image](https://github.com/Copy-Fox/Study/assets/154932134/f205ba18-619b-4023-b95e-105b5b478add)

    다층 퍼셉트론(MultiLayer Perceptron, MLP): 입력층과 출력층 사이에 1개 이상의 은닉층(hidden layer)이 존재하는 퍼셉트론

        단층 퍼셉트론의 한계를 뛰어넘어 복잡한 문제를 해결하기 위해 은닉층을 활용함.

        2개 이상의 은닉층을 가지면 심층 신경망(Deep Neural Network, DNN)이라고 함.

        이 심층 신경망으로 학습을 하는 것을 딥러닝 이라고 함.

딥 신경망

![Untitled](https://github.com/Copy-Fox/Study/assets/154932134/38acaa85-9157-4108-9a91-2118224ab7fc)

    가중치: 입력값이 결과에 미치는 영향력 정도

    가중합: 입력값을 가중치에 곱한 후 그 값들을 전부 더한 값(이 값은 활성화 함수에 입력값으로 들어가기 때문에 전달함수 라고도 함.)
    
    활성화 함수: 전달함수에서 받은 값을 특정 기준에 따라 변화해서 출력하는 비선형 함수
    
![Untitled (1)](https://github.com/Copy-Fox/Study/assets/154932134/303d0e60-d063-4fb6-a652-74c94f09199a)

시그모이드 함수: 미분이 가능한 계단 함수. 

                이진 문제에 적합함. 
                
                하지만 복잡한 신경망일 경우 기울기가 점점 사라지는 문제가 있음.
                
![Untitled (2)](https://github.com/Copy-Fox/Study/assets/154932134/12f4330f-abf0-49f4-995a-7ea188a2e6de)

하이퍼볼릭 탄젠트 함수: 시그모이드 함수(0~1)를 늘려서 (-1~1)로 만든 함수. 

                        마찬가지로 기울기 소멸 문제가 있음.
                        
![Untitled (3)](https://github.com/Copy-Fox/Study/assets/154932134/6012b648-de9d-4f8f-8502-f2c6041cb23f)

렐루 함수: 결과 값이 음수면 0, 양수면 그 값을 출력하는 함수.

           경사하강법에 영향을 주지 않아 학습속도가 빠르고 기울기 소멸 문제가 발생하지 않음.

           주로 은닉층에 사용함.

           하지만 음수값이 전부 0으로 변환 되기 때문에 학습능력이 감소함.

![Untitled (4)](https://github.com/Copy-Fox/Study/assets/154932134/30343b42-e881-4e68-94fc-b4408231c8ae)           

리키 렐루 함수: 렐루 함수에서 음수값이 0으로 변환되는 문제를 해결하는 함수.

                음수값이 들어오면 0이 아닌 매우 작은 수를 반환함.

소프트 맥스 함수: 입력값을 0~1사이의 값으로 정규화하여 출력하는 함수.

                 출력값들의 합이 1이기 때문에 출력값들 간의 확률을 구할 수 있음.

                 보통 출력층에서 많이 사용함.

손실 함수: 학습을 통해 얻은 값과 실제값 사이의 오차를 계산하는 함수.

           이 함수의 미분값을 통해 경사하강법을 할 수 있음.

           대표적인 손실함수에는 평균 제곱 오차(Mean Squared Error, MSE)와 크로스 엔트로피 오차(Cross Entropy Error, CEE)가 존재

           평균 제곱 오차: 예측값과 실제값을 빼서 그 값을 제곱하는 함수.

                           제곱을 통해 음수 값이 발생하지 않고 손실 정도를 확인하기 더 쉬워짐.

![Untitled (5)](https://github.com/Copy-Fox/Study/assets/154932134/f319fbba-d3b1-4018-81b3-6e68ddb17f02)

지역 최소점: 손실함수의 미분값이 아주 작은 지점. 

             하지만 이 값은 여러 개가 있을 수 있음.

전역 최소점: 지역 최소점 중에서 가장 작은 지점.
                          
               

