신경망: 머신러닝 중 하나, 최근에는 신경망(딥러닝)이 다른 머신러닝보디 뛰어나다는 평가를 받고 있음.

초기 신경망

    퍼셉트론: 다수의 입력 -> 하나의 결과를 나타내는 구조, 가중치를 조절해서 답을 찾음(가중치가 높으면 그 입력값이 중요하다는 의미)
              
              위의 결과 출력값이 특정 임계값을 넘을 경우 1 아닐 경우 0으로 나옴.
              
              그 외에 편향과 활성화 함수가 존재함.

    단층 퍼셉트론(Single-Layer Perceptron): 입력층과 출력층으로 구성된 단순한 퍼셉트론

                  단층 퍼셉트론은 xor게이트를 표현할 수 없어서 딥러닝에 적합하지 않음.

![image](https://github.com/Copy-Fox/Study/assets/154932134/f205ba18-619b-4023-b95e-105b5b478add)

    다층 퍼셉트론(MultiLayer Perceptron, MLP): 입력층과 출력층 사이에 1개 이상의 은닉층(hidden layer)이 존재하는 퍼셉트론

        단층 퍼셉트론의 한계를 뛰어넘어 복잡한 문제를 해결하기 위해 은닉층을 활용함.

        2개 이상의 은닉층을 가지면 심층 신경망(Deep Neural Network, DNN)이라고 함.

        이 심층 신경망으로 학습을 하는 것을 딥러닝 이라고 함.

딥 신경망

![Untitled](https://github.com/Copy-Fox/Study/assets/154932134/38acaa85-9157-4108-9a91-2118224ab7fc)

    가중치: 입력값이 결과에 미치는 영향력 정도

    가중합: 입력값을 가중치에 곱한 후 그 값들을 전부 더한 값(이 값은 활성화 함수에 입력값으로 들어가기 때문에 전달함수 라고도 함.)
    
    활성화 함수: 전달함수에서 받은 값을 특정 기준에 따라 변화해서 출력하는 비선형 함수
    
![Untitled (1)](https://github.com/Copy-Fox/Study/assets/154932134/303d0e60-d063-4fb6-a652-74c94f09199a)

시그모이드 함수: 미분이 가능한 계단 함수. 

                이진 문제에 적합함. 
                
                하지만 복잡한 신경망일 경우 기울기가 점점 사라지는 문제가 있음.
                
![Untitled (2)](https://github.com/Copy-Fox/Study/assets/154932134/12f4330f-abf0-49f4-995a-7ea188a2e6de)

하이퍼볼릭 탄젠트 함수: 시그모이드 함수(0~1)를 늘려서 (-1~1)로 만든 함수. 

                        마찬가지로 기울기 소멸 문제가 있음.
                        
![Untitled (3)](https://github.com/Copy-Fox/Study/assets/154932134/6012b648-de9d-4f8f-8502-f2c6041cb23f)

렐루 함수: 결과 값이 음수면 0, 양수면 그 값을 출력하는 함수.

           경사하강법에 영향을 주지 않아 학습속도가 빠르고 기울기 소멸 문제가 발생하지 않음.

           주로 은닉층에 사용함.

           하지만 음수값이 전부 0으로 변환 되기 때문에 학습능력이 감소함.

![Untitled (4)](https://github.com/Copy-Fox/Study/assets/154932134/30343b42-e881-4e68-94fc-b4408231c8ae)           

리키 렐루 함수: 렐루 함수에서 음수값이 0으로 변환되는 문제를 해결하는 함수.

                음수값이 들어오면 0이 아닌 매우 작은 수를 반환함.

소프트 맥스 함수: 입력값을 0~1사이의 값으로 정규화하여 출력하는 함수.

                 출력값들의 합이 1이기 때문에 출력값들 간의 확률을 구할 수 있음.

                 보통 출력층에서 많이 사용함.

손실 함수: 학습을 통해 얻은 값과 실제값 사이의 오차를 계산하는 함수.

           이 함수의 미분값을 통해 경사하강법을 할 수 있음.

           대표적인 손실함수에는 평균 제곱 오차(Mean Squared Error, MSE)와 크로스 엔트로피 오차(Cross Entropy Error, CEE)가 존재

           평균 제곱 오차: 예측값과 실제값을 빼서 그 값을 제곱하는 함수.

                           제곱을 통해 음수 값이 발생하지 않고 손실 정도를 확인하기 더 쉬워짐.

![Untitled (5)](https://github.com/Copy-Fox/Study/assets/154932134/f319fbba-d3b1-4018-81b3-6e68ddb17f02)

지역 최소점: 손실함수의 미분값이 아주 작은 지점. 

             하지만 이 값은 여러 개가 있을 수 있음.

전역 최소점: 지역 최소점 중에서 가장 작은 지점.

![Untitled (6)](https://github.com/Copy-Fox/Study/assets/154932134/380143ef-58c7-446c-9a6d-56f003cca029)

순전파: 입력층 -> 은닉층 -> 출력층 순으로 진행되는 부분

역전파: 출력층 -> 은닉층 -> 입력층 순으로 진행되는 부분

        손실을 줄이기 위해 가중치를 수정할 때 순서임.

딥러닝의 문제점

    1. 과적합 문제: 훈련 데이터를 과하게 학습해서 오차가 증가하는 문제
    
                    dropout 기능을 사용해서 일부 노드를 제외시켜서 해결함.

    2. 기울기 소멸 문제: 출력층에서 은닉층으로 손실을 줄이는 과정에서 점점 0으로 수렴되는 문제'

                        문제를 일으키는 활성화 함수(시그모이드, 하이퍼볼릭 탄젠트)를 사용하지 않음으로 해결함.

    3. 성능 저하 문제: 경사하강법을 하는 과정에서 성능이 떨어지는 문제

            배치 경사 하강법 (Batch Gradient Descent, BGD): 손실함수의 기울기를 한 번에 계산하는 방식

                                                            한 번에 계산하기 때문에 오래걸림.
                                                            
![Untitled (7)](https://github.com/Copy-Fox/Study/assets/154932134/d9cccc4d-11b1-4ae9-bf6f-e99cddd72aca)

            확률적 경사 하강법 (Stochastic Gradient Descent, SGD): 임의로 선택한 데이터의 손실함수를 계산하는 방식.

                                                                   배치 경사 하강법보다 속도가 빠름, 정확도는 떨어짐.
                                                                   
![Untitled (8)](https://github.com/Copy-Fox/Study/assets/154932134/faa7a84d-0b08-4b5a-822e-b61f4c028dd8)

            미니 배치 경사 하강법: 위의 두 방법의 절충안.

                                   데이터들을 여러 개의 배치에 나누어 각각 계산함.

                                   확률적 경사 하강법 보다 안정성이 올라감.

            옵티마이저: 확률적 경사 하강법의 불안정한 부분을 해결하는 함수.

                        주로 학습속도와 운동량을 조절함.

                        보편적으로 '아담'을 많이 사용함.

합성곱 신경망(Convolutional Neural Network, CNN): 이미지를 국소적으로 계산해서 시간과 자원을 절약하면서 세밀하게 분석하는 신경망

CNN 가이드 라인

    1. 입력크기: 고정된 크기 (ex: (28,28,1),(224,224,3))

    2. 초기 합성곱 레이어: 필터 크기 작게 시작 -> 점점 크게

    3. 풀링 레이어: 일반적으로 (2x2 ,스트라이드:2) 사용

    4. 점진적 복잡성 증가: 필터 수 작게 시작 -> 점점 크게

    5. 활성화 함수: Relu 사용

    6. 드롭아웃: 0.5 정도, 주로 dense 레이어 앞에 사용

    7. 배치 정규화: 합성곱 레이어 와 활성화 함수 사이에 배치, 안정성과 학습속도 향상

    8. Flatten 및 Dense 레이어: 1차원 벡터로 형성(Flatten), 최종 레이어는 Softmax사용

    9. 데이터 증강: 기존 데이터를 회전,이동,확대 등을 사용해서 데이터 증강 

CNN의 특징

    -합성곱층은 기본적으로 이미지, 영상 처리에 사용

